FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535520000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000005960464480000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=26 4 3 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.50000000000000000000e+003) (1, 1.89702022075653080000e+000) (2, 1.01685738563537600000e+000) (3, 2.25033354759216310000e+000) (4, 1.50000000000000000000e+003) (5, 2.51805233955383300000e+000) (6, 7.46724724769592290000e-001) (7, -3.39887745212763550000e-004) (8, 4.21173274517059330000e-001) (9, 8.61407279968261720000e-001) (10, 1.50000000000000000000e+003) (11, 2.22693935036659240000e-001) (12, -1.49634391069412230000e-001) (13, 8.04689586162567140000e-001) (14, 5.84750652313232420000e-001) (15, 1.50000000000000000000e+003) (16, -3.80500912666320800000e-001) (17, 3.31844925880432130000e-001) (18, 2.87186622619628910000e-001) (19, 2.35024237632751460000e+000) (20, 1.50000000000000000000e+003) (21, 1.50000000000000000000e+003) (22, 1.50000000000000000000e+003) (23, 2.81870174407958980000e+000) (24, 1.50000000000000000000e+003) (25, 8.04449796676635740000e-001) (0, -1.50000000000000000000e+003) (1, -1.66930377483367920000e+000) (2, -1.46907544136047360000e+000) (3, -1.85985708236694340000e+000) (4, -1.50000000000000000000e+003) (5, -3.91068398952484130000e-001) (6, 2.58314847946166990000e+000) (7, -6.32320523262023930000e-001) (8, 4.80711966753005980000e-001) (9, 2.13513776659965520000e-001) (10, -1.50000000000000000000e+003) (11, 1.34703230857849120000e+000) (12, 6.01504147052764890000e-001) (13, 4.84386968612670900000e+000) (14, -6.90949037671089170000e-002) (15, -1.50000000000000000000e+003) (16, -1.50000000000000000000e+003) (17, 3.33419978618621830000e-001) (18, 5.13106100261211400000e-002) (19, -6.89651584625244140000e+000) (20, -1.50000000000000000000e+003) (21, -1.50000000000000000000e+003) (22, -1.50000000000000000000e+003) (23, 1.78368628025054930000e+000) (24, -1.50000000000000000000e+003) (25, 8.55112493038177490000e-001) (0, 4.86633256077766420000e-002) (1, 7.21761658787727360000e-002) (2, 1.03727839887142180000e-001) (3, 2.91399657726287840000e-001) (4, 5.54600358009338380000e-001) (5, -4.78860408067703250000e-001) (6, -5.57168543338775630000e-001) (7, -1.84946358203887940000e-002) (8, 1.99988082051277160000e-001) (9, 4.46956694126129150000e-001) (10, -4.16541785001754760000e-001) (11, -9.10373032093048100000e-001) (12, -5.60299046337604520000e-002) (13, 2.17860788106918330000e-001) (14, 6.20775640010833740000e-001) (15, -5.27415394783020020000e-001) (16, -1.14263927936553960000e+000) (17, 1.27635014057159420000e+000) (18, 1.17788517475128170000e+000) (19, 1.26573896408081050000e+000) (20, -1.36447966098785400000e+000) (21, -1.45247998046875000000e+003) (22, 3.34597885608673100000e-001) (23, 2.82522082328796390000e+000) (24, 1.81828916072845460000e+000) (25, 1.13798528909683230000e-001) (26, 2.66274482011795040000e-001) (27, -3.63542288541793820000e-001) (28, -3.14244818687438960000e+000) (29, 3.60865652561187740000e-001) (26, -3.41689610481262210000e+000) (27, 8.85506093502044680000e-001) (28, 3.85601520538330080000e-001) (29, 1.84285676479339600000e+000) 
